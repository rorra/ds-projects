{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928fe27-e5b2-4fd5-8920-fe6e4cdc3e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64e0f4-a43c-4461-a2cd-4e77c5b47f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    datasets,\n",
    "    layers,\n",
    "    models,\n",
    "    optimizers,\n",
    "    activations,\n",
    "    metrics,\n",
    "    callbacks,\n",
    ")\n",
    "\n",
    "from utils import display, sample_batch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f7a88c-4293-45e0-b90e-b89efdf68ee9",
   "metadata": {},
   "source": [
    "# 0. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d17da4-52f9-4776-b1a3-27b7e0e4d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 1\n",
    "STEP_SIZE = 10\n",
    "STEPS = 60\n",
    "NOISE = 0.005\n",
    "ALPHA = 0.1\n",
    "GRADIENT_CLIP = 0.03\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 8192\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 60\n",
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae48258-7b4a-4e34-ba8f-a0fdcbe09953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "(x_train, _), (x_test, _) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dcec46-a71d-4579-90e8-219b7c481fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "\n",
    "def preprocess(imgs):\n",
    "    \"\"\"\n",
    "    Normalize and reshape the images\n",
    "    \"\"\"\n",
    "    imgs = (imgs.astype(\"float32\") - 127.5) / 127.5\n",
    "    imgs = np.pad(imgs, ((0, 0), (2, 2), (2, 2)), constant_values=-1.0)\n",
    "    imgs = np.expand_dims(imgs, -1)\n",
    "    return imgs\n",
    "\n",
    "\n",
    "x_train = preprocess(x_train)\n",
    "x_test = preprocess(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abe457-0df1-4b3e-8c2b-8e72746283cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).batch(BATCH_SIZE)\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f721c-64b9-459b-b117-bbe0252e55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some items of clothing from the training set\n",
    "train_sample = sample_batch(x_train)\n",
    "display(train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6a30c3-1a5b-40cf-a59e-e9adafbeb1b5",
   "metadata": {},
   "source": [
    "# 2. Build the EBM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f14d3bc-4682-410a-8e48-ec6a079b83ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
    "x = layers.Conv2D(\n",
    "    16, kernel_size=5, strides=2, padding=\"same\", activation=activations.swish\n",
    ")(ebm_input)\n",
    "x = layers.Conv2D(\n",
    "    32, kernel_size=3, strides=2, padding=\"same\", activation=activations.swish\n",
    ")(x)\n",
    "x = layers.Conv2D(\n",
    "    64, kernel_size=3, strides=2, padding=\"same\", activation=activations.swish\n",
    ")(x)\n",
    "x = layers.Conv2D(\n",
    "    64, kernel_size=3, strides=2, padding=\"same\", activation=activations.swish\n",
    ")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(64, activation=activations.swish)(x)\n",
    "ebm_output = layers.Dense(1)(x)\n",
    "model = models.Model(ebm_input, ebm_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa61eff-68da-493b-b378-299472a2aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    model.load_weights(\"./models/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6db9b0-aa75-4913-9a89-22deafd369f7",
   "metadata": {},
   "source": [
    "# 2. Set up a Langevin sampler function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4da36-5638-4ead-bfd7-91d95e84c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate samples using Langevin Dynamics\n",
    "def generate_samples(\n",
    "    model, inp_imgs, steps, step_size, noise, return_img_per_step=False\n",
    "):\n",
    "    imgs_per_step = []\n",
    "    for _ in range(steps):\n",
    "        inp_imgs += tf.random.normal(inp_imgs.shape, mean=0, stddev=noise)\n",
    "        inp_imgs = tf.clip_by_value(inp_imgs, -1.0, 1.0)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(inp_imgs)\n",
    "            out_score = model(inp_imgs)\n",
    "        grads = tape.gradient(out_score, inp_imgs)\n",
    "        grads = tf.clip_by_value(grads, -GRADIENT_CLIP, GRADIENT_CLIP)\n",
    "        inp_imgs += step_size * grads\n",
    "        inp_imgs = tf.clip_by_value(inp_imgs, -1.0, 1.0)\n",
    "        if return_img_per_step:\n",
    "            imgs_per_step.append(inp_imgs)\n",
    "    if return_img_per_step:\n",
    "        return tf.stack(imgs_per_step, axis=0)\n",
    "    else:\n",
    "        return inp_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e76b55-86e1-4d09-ae22-a337a19abd1d",
   "metadata": {},
   "source": [
    "# 3. Set up a buffer to store examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1dc4a9-9246-438b-ba59-b50338f8d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.examples = [\n",
    "            tf.random.uniform(shape=(1, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)) * 2\n",
    "            - 1\n",
    "            for _ in range(BATCH_SIZE)\n",
    "        ]\n",
    "\n",
    "    def sample_new_exmps(self, steps, step_size, noise):\n",
    "        n_new = np.random.binomial(BATCH_SIZE, 0.05)\n",
    "        rand_imgs = (\n",
    "            tf.random.uniform((n_new, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)) * 2 - 1\n",
    "        )\n",
    "        old_imgs = tf.concat(\n",
    "            random.choices(self.examples, k=BATCH_SIZE - n_new), axis=0\n",
    "        )\n",
    "        inp_imgs = tf.concat([rand_imgs, old_imgs], axis=0)\n",
    "        inp_imgs = generate_samples(\n",
    "            self.model, inp_imgs, steps=steps, step_size=step_size, noise=noise\n",
    "        )\n",
    "        self.examples = tf.split(inp_imgs, BATCH_SIZE, axis=0) + self.examples\n",
    "        self.examples = self.examples[:BUFFER_SIZE]\n",
    "        return inp_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4b184-1fad-432c-afdf-917d3407b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EBM(models.Model):\n",
    "    def __init__(self):\n",
    "        super(EBM, self).__init__()\n",
    "        self.model = model\n",
    "        self.buffer = Buffer(self.model)\n",
    "        self.alpha = ALPHA\n",
    "        self.loss_metric = metrics.Mean(name=\"loss\")\n",
    "        self.reg_loss_metric = metrics.Mean(name=\"reg\")\n",
    "        self.cdiv_loss_metric = metrics.Mean(name=\"cdiv\")\n",
    "        self.real_out_metric = metrics.Mean(name=\"real\")\n",
    "        self.fake_out_metric = metrics.Mean(name=\"fake\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.loss_metric,\n",
    "            self.reg_loss_metric,\n",
    "            self.cdiv_loss_metric,\n",
    "            self.real_out_metric,\n",
    "            self.fake_out_metric,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, real_imgs):\n",
    "        real_imgs += tf.random.normal(\n",
    "            shape=tf.shape(real_imgs), mean=0, stddev=NOISE\n",
    "        )\n",
    "        real_imgs = tf.clip_by_value(real_imgs, -1.0, 1.0)\n",
    "        fake_imgs = self.buffer.sample_new_exmps(\n",
    "            steps=STEPS, step_size=STEP_SIZE, noise=NOISE\n",
    "        )\n",
    "        inp_imgs = tf.concat([real_imgs, fake_imgs], axis=0)\n",
    "        with tf.GradientTape() as training_tape:\n",
    "            real_out, fake_out = tf.split(self.model(inp_imgs), 2, axis=0)\n",
    "            cdiv_loss = tf.reduce_mean(fake_out, axis=0) - tf.reduce_mean(\n",
    "                real_out, axis=0\n",
    "            )\n",
    "            reg_loss = self.alpha * tf.reduce_mean(\n",
    "                real_out**2 + fake_out**2, axis=0\n",
    "            )\n",
    "            loss = cdiv_loss + reg_loss\n",
    "        grads = training_tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables)\n",
    "        )\n",
    "        self.loss_metric.update_state(loss)\n",
    "        self.reg_loss_metric.update_state(reg_loss)\n",
    "        self.cdiv_loss_metric.update_state(cdiv_loss)\n",
    "        self.real_out_metric.update_state(tf.reduce_mean(real_out, axis=0))\n",
    "        self.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis=0))\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, real_imgs):\n",
    "        batch_size = real_imgs.shape[0]\n",
    "        fake_imgs = (\n",
    "            tf.random.uniform((batch_size, IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
    "            * 2\n",
    "            - 1\n",
    "        )\n",
    "        inp_imgs = tf.concat([real_imgs, fake_imgs], axis=0)\n",
    "        real_out, fake_out = tf.split(self.model(inp_imgs), 2, axis=0)\n",
    "        cdiv = tf.reduce_mean(fake_out, axis=0) - tf.reduce_mean(\n",
    "            real_out, axis=0\n",
    "        )\n",
    "        self.cdiv_loss_metric.update_state(cdiv)\n",
    "        self.real_out_metric.update_state(tf.reduce_mean(real_out, axis=0))\n",
    "        self.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis=0))\n",
    "        return {m.name: m.result() for m in self.metrics[2:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d577e507-6a97-41ce-b721-20014e80770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm = EBM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f78f50-6a5f-40d2-b7e6-910e2a95b705",
   "metadata": {},
   "source": [
    "# 3. Train the EBM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155e5dc-126e-4595-b72a-85628998e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model\n",
    "ebm.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=LEARNING_RATE), run_eagerly=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f205d54-e7b2-4944-903a-ddad15a07d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "\n",
    "class ImageGenerator(callbacks.Callback):\n",
    "    def __init__(self, num_img):\n",
    "        self.num_img = num_img\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_imgs = (\n",
    "            np.random.uniform(\n",
    "                size=(self.num_img, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "            )\n",
    "            * 2\n",
    "            - 1\n",
    "        )\n",
    "        generated_images = generate_samples(\n",
    "            ebm.model,\n",
    "            start_imgs,\n",
    "            steps=1000,\n",
    "            step_size=STEP_SIZE,\n",
    "            noise=NOISE,\n",
    "            return_img_per_step=False,\n",
    "        )\n",
    "        generated_images = generated_images.numpy()\n",
    "        display(\n",
    "            generated_images,\n",
    "            save_to=\"./output/generated_img_%03d.png\" % (epoch),\n",
    "        )\n",
    "\n",
    "        example_images = tf.concat(\n",
    "            random.choices(ebm.buffer.examples, k=10), axis=0\n",
    "        )\n",
    "        example_images = example_images.numpy()\n",
    "        display(\n",
    "            example_images, save_to=\"./output/example_img_%03d.png\" % (epoch)\n",
    "        )\n",
    "\n",
    "\n",
    "image_generator_callback = ImageGenerator(num_img=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bcb63d-7f37-4328-b390-5c4b0803ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModel(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        model.save_weights(\"./models/model.h5\")\n",
    "\n",
    "\n",
    "save_model_callback = SaveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195c674-e073-42eb-ad80-81eb601ca1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm.fit(\n",
    "    x_train,\n",
    "    shuffle=True,\n",
    "    epochs=60,\n",
    "    validation_data=x_test,\n",
    "    callbacks=[\n",
    "        save_model_callback,\n",
    "        tensorboard_callback,\n",
    "        image_generator_callback,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a6608-210e-4a5d-9e70-5b83c462ba7a",
   "metadata": {},
   "source": [
    "# 4. Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d69bbeb-41b0-4c67-832e-86e5aaf85155",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_imgs = (\n",
    "    np.random.uniform(size=(10, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)) * 2 - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4bd5f0-ef8c-4bd4-ac15-0fdf965d85b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(start_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d08546-6d98-424f-aeaa-41dfef7d56fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_img = generate_samples(\n",
    "    ebm.model,\n",
    "    start_imgs,\n",
    "    steps=1000,\n",
    "    step_size=STEP_SIZE,\n",
    "    noise=NOISE,\n",
    "    return_img_per_step=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db400d5-00d1-4136-9175-4aa681f57042",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(gen_img[-1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a3761-f816-42db-b50d-30f954c5f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for i in [0, 1, 3, 5, 10, 30, 50, 100, 300, 999]:\n",
    "    imgs.append(gen_img[i].numpy()[6])\n",
    "\n",
    "display(np.array(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2be7e5-a136-4ec3-9f38-1e76e6322675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
