{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac1098a0b8624d4e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 21:55:11.900453: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-15 21:55:11.900475: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-15 21:55:11.901125: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers, callbacks\n",
    "\n",
    "from utils import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb86ce5ffd74a89",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 0. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9c21f97184e470",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 16\n",
    "PIXEL_LEVELS = 4\n",
    "N_FILTERS = 128\n",
    "RESIDUAL_BLOCKS = 5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9a37676392f6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 1. Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcaf09c46559bf58",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "(x_train, _), (_, _) = datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e88fa0241445073",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 21:55:13.981584: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-15 21:55:13.991143: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-15 21:55:14.003274: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-15 21:55:14.014275: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-15 21:55:14.020641: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-15 21:55:14.029900: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-15 21:55:14.164111: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-15 21:55:14.165513: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-15 21:55:14.166765: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-15 21:55:14.167936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6881 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0a:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "def preprocess(imgs_int):\n",
    "    imgs_int = np.expand_dims(imgs_int, -1)\n",
    "    imgs_int = tf.image.resize(imgs_int, (IMAGE_SIZE, IMAGE_SIZE)).numpy()\n",
    "    imgs_int = (imgs_int / (256 / PIXEL_LEVELS)).astype(int)\n",
    "    imgs = imgs_int.astype(\"float32\")\n",
    "    imgs = imgs / PIXEL_LEVELS\n",
    "    return imgs, imgs_int\n",
    "\n",
    "\n",
    "input_data, output_data = preprocess(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8977cef9a307106a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show some items of clothing from the training set\n",
    "display(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e646c25d56bba06f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 2. Build the PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "994ceb34dba4908e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The first layer is the PixelCNN layer. This layer simply builds on the 2D convolutional layer, but includes masking.\n",
    "class MaskedConv2D(layers.Layer):\n",
    "    def __init__(self, mask_type, **kwargs):\n",
    "        super(MaskedConv2D, self).__init__()\n",
    "        self.mask_type = mask_type\n",
    "        self.conv = layers.Conv2D(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Build the conv2d layer to initialize kernel variables\n",
    "        self.conv.build(input_shape)\n",
    "        # Use the initialized kernel to create the mask\n",
    "        kernel_shape = self.conv.kernel.get_shape()\n",
    "        self.mask = np.zeros(shape=kernel_shape)\n",
    "        self.mask[: kernel_shape[0] // 2, ...] = 1.0\n",
    "        self.mask[kernel_shape[0] // 2, : kernel_shape[1] // 2, ...] = 1.0\n",
    "        if self.mask_type == \"B\":\n",
    "            self.mask[kernel_shape[0] // 2, kernel_shape[1] // 2, ...] = 1.0\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.conv.kernel.assign(self.conv.kernel * self.mask)\n",
    "        return self.conv(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5077b3670e6f463",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(layers.Layer):\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv2D(\n",
    "            filters=filters // 2, kernel_size=1, activation=\"relu\"\n",
    "        )\n",
    "        self.pixel_conv = MaskedConv2D(\n",
    "            mask_type=\"B\",\n",
    "            filters=filters // 2,\n",
    "            kernel_size=3,\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.conv2 = layers.Conv2D(\n",
    "            filters=filters, kernel_size=1, activation=\"relu\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pixel_conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return layers.add([inputs, x])\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95379fcb0330b3bb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 16, 16, 1)]       0         \n",
      "                                                                 \n",
      " masked_conv2d (MaskedConv2  (None, 16, 16, 128)       6400      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " residual_block (ResidualBl  (None, 16, 16, 128)       53504     \n",
      " ock)                                                            \n",
      "                                                                 \n",
      " residual_block_1 (Residual  (None, 16, 16, 128)       53504     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " residual_block_2 (Residual  (None, 16, 16, 128)       53504     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " residual_block_3 (Residual  (None, 16, 16, 128)       53504     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " residual_block_4 (Residual  (None, 16, 16, 128)       53504     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " masked_conv2d_6 (MaskedCon  (None, 16, 16, 128)       16512     \n",
      " v2D)                                                            \n",
      "                                                                 \n",
      " masked_conv2d_7 (MaskedCon  (None, 16, 16, 128)       16512     \n",
      " v2D)                                                            \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 16, 16, 4)         516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 307460 (1.17 MB)\n",
      "Trainable params: 307460 (1.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "x = MaskedConv2D(\n",
    "    mask_type=\"A\",\n",
    "    filters=N_FILTERS,\n",
    "    kernel_size=7,\n",
    "    activation=\"relu\",\n",
    "    padding=\"same\",\n",
    ")(inputs)\n",
    "\n",
    "for _ in range(RESIDUAL_BLOCKS):\n",
    "    x = ResidualBlock(filters=N_FILTERS)(x)\n",
    "\n",
    "for _ in range(2):\n",
    "    x = MaskedConv2D(\n",
    "        mask_type=\"B\",\n",
    "        filters=N_FILTERS,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        activation=\"relu\",\n",
    "        padding=\"valid\",\n",
    "    )(x)\n",
    "\n",
    "out = layers.Conv2D(\n",
    "    filters=PIXEL_LEVELS,\n",
    "    kernel_size=1,\n",
    "    strides=1,\n",
    "    activation=\"softmax\",\n",
    "    padding=\"valid\",\n",
    ")(x)\n",
    "\n",
    "pixel_cnn = models.Model(inputs, out)\n",
    "pixel_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2d39c503c5a34",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 3. Train the PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8e3c1c9cf7f4275",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(learning_rate=0.0005)\n",
    "pixel_cnn.compile(optimizer=adam, loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc8523cc95aa3845",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "\n",
    "class ImageGenerator(callbacks.Callback):\n",
    "    def __init__(self, num_img):\n",
    "        self.num_img = num_img\n",
    "\n",
    "    def sample_from(self, probs, temperature):  # <2>\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "    def generate(self, temperature):\n",
    "        generated_images = np.zeros(\n",
    "            shape=(self.num_img,) + (pixel_cnn.input_shape)[1:]\n",
    "        )\n",
    "        batch, rows, cols, channels = generated_images.shape\n",
    "\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                for channel in range(channels):\n",
    "                    probs = self.model.predict(generated_images, verbose=0)[\n",
    "                            :, row, col, :\n",
    "                            ]\n",
    "                    generated_images[:, row, col, channel] = [\n",
    "                        self.sample_from(x, temperature) for x in probs\n",
    "                    ]\n",
    "                    generated_images[:, row, col, channel] /= PIXEL_LEVELS\n",
    "\n",
    "        return generated_images\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        generated_images = self.generate(temperature=1.0)\n",
    "        display(\n",
    "            generated_images,\n",
    "            save_to=\"./output/generated_img_%03d.png\" % (epoch),\n",
    "        )\n",
    "\n",
    "\n",
    "img_generator_callback = ImageGenerator(num_img=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c01bbd9bef6b097",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 21:55:19.318927: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-15 21:55:20.838441: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-03-15 21:55:20.905040: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-15 21:55:21.471929: I external/local_xla/xla/service/service.cc:168] XLA service 0x7667d5a25c90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-15 21:55:21.471948: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6\n",
      "2024-03-15 21:55:21.475162: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1710550521.538244  471124 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/469 [..............................] - ETA: 30:53 - loss: 1.3846WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0100s). Check your callbacks.\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.4903OK\n",
      "469/469 [==============================] - 9s 12ms/step - loss: 0.4903\n",
      "Epoch 2/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.4012OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.4011\n",
      "Epoch 3/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3865OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3865\n",
      "Epoch 4/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3802OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3802\n",
      "Epoch 5/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3761OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3762\n",
      "Epoch 6/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3722OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3722\n",
      "Epoch 7/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3695OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3695\n",
      "Epoch 8/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3667OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3668\n",
      "Epoch 9/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3645OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3645\n",
      "Epoch 10/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3625OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3624\n",
      "Epoch 11/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3605OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3604\n",
      "Epoch 12/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3581OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3581\n",
      "Epoch 13/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3567OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3567\n",
      "Epoch 14/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3552OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3551\n",
      "Epoch 15/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3535OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3535\n",
      "Epoch 16/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3525OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3525\n",
      "Epoch 17/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3513OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3512\n",
      "Epoch 18/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3505OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3505\n",
      "Epoch 19/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3494OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3493\n",
      "Epoch 20/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3484OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3483\n",
      "Epoch 21/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3477OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3477\n",
      "Epoch 22/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3469OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3470\n",
      "Epoch 23/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3460OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3460\n",
      "Epoch 24/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3455OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3455\n",
      "Epoch 25/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3450OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3449\n",
      "Epoch 26/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3441OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3440\n",
      "Epoch 27/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3437OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3437\n",
      "Epoch 28/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3430OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3430\n",
      "Epoch 29/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3424OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3424\n",
      "Epoch 30/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3421OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3421\n",
      "Epoch 31/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3413OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3414\n",
      "Epoch 32/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3410OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3410\n",
      "Epoch 33/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3405OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3404\n",
      "Epoch 34/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3401OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3400\n",
      "Epoch 35/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3398OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3398\n",
      "Epoch 36/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3394OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3394\n",
      "Epoch 37/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3387OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3387\n",
      "Epoch 38/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3387OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3387\n",
      "Epoch 39/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3381OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3381\n",
      "Epoch 40/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3378OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3377\n",
      "Epoch 41/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3375OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3374\n",
      "Epoch 42/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3371OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3371\n",
      "Epoch 43/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3369OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3368\n",
      "Epoch 44/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3367OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3367\n",
      "Epoch 45/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3362OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3363\n",
      "Epoch 46/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3361OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3360\n",
      "Epoch 47/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3356OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3356\n",
      "Epoch 48/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3354OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3354\n",
      "Epoch 49/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3351OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3351\n",
      "Epoch 50/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3347OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3347\n",
      "Epoch 51/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3346OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3346\n",
      "Epoch 52/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3344OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3344\n",
      "Epoch 53/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3341OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3341\n",
      "Epoch 54/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3339OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3339\n",
      "Epoch 55/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3336OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3336\n",
      "Epoch 56/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3335OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3335\n",
      "Epoch 57/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3332OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3332\n",
      "Epoch 58/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3330OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3330\n",
      "Epoch 59/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3328OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3328\n",
      "Epoch 60/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3325OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3325\n",
      "Epoch 61/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3326OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3325\n",
      "Epoch 62/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3323OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3323\n",
      "Epoch 63/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3319OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3319\n",
      "Epoch 64/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3319OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3319\n",
      "Epoch 65/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3316OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3316\n",
      "Epoch 66/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3314OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3314\n",
      "Epoch 67/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3314OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3313\n",
      "Epoch 68/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3312OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3311\n",
      "Epoch 69/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3310OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3310\n",
      "Epoch 70/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3308OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3308\n",
      "Epoch 71/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3306OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3306\n",
      "Epoch 72/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3304OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3304\n",
      "Epoch 73/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3301OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3301\n",
      "Epoch 74/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3301OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3301\n",
      "Epoch 75/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3298OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3298\n",
      "Epoch 76/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3300OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3300\n",
      "Epoch 77/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3297OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3297\n",
      "Epoch 78/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3296OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3296\n",
      "Epoch 79/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3294OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3294\n",
      "Epoch 80/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3292OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3292\n",
      "Epoch 81/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3289OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3289\n",
      "Epoch 82/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3290OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3289\n",
      "Epoch 83/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3290OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3290\n",
      "Epoch 84/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3289OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3289\n",
      "Epoch 85/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3285OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3285\n",
      "Epoch 86/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3284OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3284\n",
      "Epoch 87/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3285OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3285\n",
      "Epoch 88/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3283OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3282\n",
      "Epoch 89/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3278OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3279\n",
      "Epoch 90/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3280OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3279\n",
      "Epoch 91/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3278OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3279\n",
      "Epoch 92/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3278OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3277\n",
      "Epoch 93/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3278OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3278\n",
      "Epoch 94/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3274OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3274\n",
      "Epoch 95/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3272OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3272\n",
      "Epoch 96/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3272OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3271\n",
      "Epoch 97/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3273OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3273\n",
      "Epoch 98/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3271OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3271\n",
      "Epoch 99/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3270OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3270\n",
      "Epoch 100/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3268OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3268\n",
      "Epoch 101/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3268OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3268\n",
      "Epoch 102/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3264OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3265\n",
      "Epoch 103/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3265OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3266\n",
      "Epoch 104/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3265OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3264\n",
      "Epoch 105/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3263OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3262\n",
      "Epoch 106/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3263OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3263\n",
      "Epoch 107/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3261OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3261\n",
      "Epoch 108/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3260OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3260\n",
      "Epoch 109/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3259OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3259\n",
      "Epoch 110/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3259OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3259\n",
      "Epoch 111/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3257OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3257\n",
      "Epoch 112/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3257OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3257\n",
      "Epoch 113/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3256OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3256\n",
      "Epoch 114/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3255OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3255\n",
      "Epoch 115/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3254OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3254\n",
      "Epoch 116/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3253OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3252\n",
      "Epoch 117/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3253OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3253\n",
      "Epoch 118/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3252OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3251\n",
      "Epoch 119/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3250OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3251\n",
      "Epoch 120/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3249OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3249\n",
      "Epoch 121/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3248OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3248\n",
      "Epoch 122/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3248OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3248\n",
      "Epoch 123/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3248OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3248\n",
      "Epoch 124/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3247OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3248\n",
      "Epoch 125/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3247OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3246\n",
      "Epoch 126/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3245OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3245\n",
      "Epoch 127/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3246OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3246\n",
      "Epoch 128/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3242OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3242\n",
      "Epoch 129/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3242OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3243\n",
      "Epoch 130/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3241OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3241\n",
      "Epoch 131/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3242OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3242\n",
      "Epoch 132/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3241OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3240\n",
      "Epoch 133/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3241OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3241\n",
      "Epoch 134/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3238OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3238\n",
      "Epoch 135/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3238OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3238\n",
      "Epoch 136/150\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.3237OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3237\n",
      "Epoch 137/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3237OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3237\n",
      "Epoch 138/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3235OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3235\n",
      "Epoch 139/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3236OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3236\n",
      "Epoch 140/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3237OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3236\n",
      "Epoch 141/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3233OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3233\n",
      "Epoch 142/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3236OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3236\n",
      "Epoch 143/150\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.3232OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3233\n",
      "Epoch 144/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3233OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3233\n",
      "Epoch 145/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3231OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3232\n",
      "Epoch 146/150\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.3231OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3231\n",
      "Epoch 147/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3231OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3231\n",
      "Epoch 148/150\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.3229OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3229\n",
      "Epoch 149/150\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.3229OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3229\n",
      "Epoch 150/150\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.3229OK\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x766b7ea9a9e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_cnn.fit(\n",
    "    input_data,\n",
    "    output_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[tensorboard_callback, img_generator_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79635b63e66ec96",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 4. Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c05b160324e7c6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "generated_images = img_generator_callback.generate(temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77bb17164275b08d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(generated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c36d7768cc706",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
